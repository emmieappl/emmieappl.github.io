[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Data Viz",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Emmie Appl",
    "section": "",
    "text": "Hello there! I am a Pomona College class of 2025 Molecular Biology major and Japanese minor. Currently, I am working as an undergraduate researcher in the Liu Lab studying mannitol-driven biofilm formation in Vibrio cholerae. Outside of the lab, I am a captain for the Pomona-Pitzer Swimming & Diving team."
  },
  {
    "objectID": "DataViz.html",
    "href": "DataViz.html",
    "title": "Data Viz",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Cheese.html",
    "href": "Cheese.html",
    "title": "Cheese",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Valentine's.html",
    "href": "Valentine's.html",
    "title": "Valentine’s Day",
    "section": "",
    "text": "Show the code\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n\n\n---- Compiling #TidyTuesday Information for 2024-02-13 ----\n--- There are 3 files available ---\n\n\n── Downloading files ───────────────────────────────────────────────────────────\n\n  1 of 3: \"historical_spending.csv\"\n  2 of 3: \"gifts_age.csv\"\n  3 of 3: \"gifts_gender.csv\"\n\n\nShow the code\nhistorical_spending &lt;- tuesdata$historical_spending\n\nlibrary(ggplot2)\n\nggplot(historical_spending, aes(x = Year, y = PerPerson)) +\n  geom_col() +\n  labs(\n    x = \"Year\",\n    y = \"Average Money Spent per Person ($)\"\n  )"
  },
  {
    "objectID": "UKdrugs.html",
    "href": "UKdrugs.html",
    "title": "European Drug Development",
    "section": "",
    "text": "This data set includes the drugs considered for authorization in the European Union between 1995 and 2023. A generic medicine is developed to be the same as a medicine that has already been authorized, with the same active ingredient(s) and dosage(s) to treat the same condition(s). Accelerated assessment indicates that the medicine is a major interest for public health, so its timeframe for review is 150 evaluation days rather than 210.\n\n\nShow the code\ntuesdata &lt;- tidytuesdayR::tt_load('2023-03-14')\n\ndrugs &lt;- tuesdata$drugs\n\nlibrary(ggplot2)\n\nggplot(drugs, aes(y = generic)) +\n  geom_bar(aes(fill = accelerated_assessment)) +\n  labs(\n    y = \"Generic Medicine\",\n    x = \"Drug Count\",\n    fill = \"Accelerated Assessment\"\n  )\n\n\n\n\n\n\n\n\n\nTidyTuesday 2023-03-14\nData from European Medicines Agency via Miquel Anglada Girotto"
  },
  {
    "objectID": "Valentines.html",
    "href": "Valentines.html",
    "title": "Valentine’s Day Retail",
    "section": "",
    "text": "This data was collected by the National Retail Federation in the United States and explores how consumers celebrate Valentine’s Day from 2010 to 2022.\n\n\nShow the code\ntuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n\nhistorical_spending &lt;- tuesdata$historical_spending\n\nlibrary(ggplot2)\n\nggplot(historical_spending, aes(x = Year, y = PerPerson)) +\n  geom_col() +\n  labs(\n    x = \"Year\",\n    y = \"Average Money Spent per Person ($)\"\n  )\n\n\n\n\n\n\n\n\n\nTidyTuesday 2024-02-13\nData from the National Retail Federation via Sunja aa Kaggle"
  },
  {
    "objectID": "StringAnalysis.html",
    "href": "StringAnalysis.html",
    "title": "String Analyses",
    "section": "",
    "text": "Firstly, I was curious as to which of the six main Friends characters had the most lines overall throughout the entire Friends series. The table below shows that Rachel had the most lines throughout the running of the show while Phoebe had the fewest.\n\n\nShow the code\nlibrary(friends)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\nfriends |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_lines = n()) |&gt;\n  arrange(desc(num_lines)) |&gt;\n  head()\n\n\n# A tibble: 6 × 2\n  speaker        num_lines\n  &lt;chr&gt;              &lt;int&gt;\n1 Rachel Green        9307\n2 Ross Geller         9157\n3 Chandler Bing       8462\n4 Monica Geller       8440\n5 Joey Tribbiani      8171\n6 Phoebe Buffay       7499\n\n\n\n\nShow the code\nquestions &lt;- friends |&gt;\n  select(text, speaker) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  mutate(totalquestions = str_count(text, \"\\\\?\")) |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_questions = sum(totalquestions))\n  \ntotalnumber &lt;- friends |&gt;\n  select(text, speaker) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  mutate(totalphrases = str_count(text, \"\\\\?(?=\\\\s|$)|\\\\.(?=\\\\s|$)|\\\\!(?=\\\\s|$)\")) |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_phrases = sum(totalphrases))\n  \nquestionsandtotal &lt;- full_join(questions, totalnumber) |&gt;\n  mutate(propquestions = num_questions/num_phrases) |&gt;\n  arrange(desc(propquestions))\n\nggplot(questionsandtotal, aes(x = speaker, y = propquestions)) +\n  geom_col(fill = \"thistle\") +\n  labs(\n    x = \"Character\",\n    y = \"Proportion of Sentences that are Questions\",\n    title = expression(paste(\"Which \", italic(\"Friends \"), \"Character is the Most Inquisitive?\"))\n  )\n\n\n\n\n\n\n\n\n\nThe next question I wanted to answer using the Friends data set was “which character asked the most questions?” during their time in the show. I went about answering this question by counting the number of questions each character asked, as well as the number of sentences each character spoke. By dividing the number of questions by the number of sentences (including questions), I calculated the proportion of sentences that were questions for each character. From this, I learned that Monica had the largest proportion of questions while Phoebe had the smallest.\n\n\nShow the code\nwithyear &lt;- left_join(friends, friends_info, by = c(\"season\", \"episode\")) |&gt;\n  mutate(year = as.factor(str_sub(air_date, 1, 4))) |&gt;\n  select(text, speaker, year) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  group_by(speaker, year) |&gt;\n  summarize(lines = n())\n\nggplot(withyear, aes(x = year, y = lines, group = speaker)) +\n  geom_line(aes(color = speaker)) +\n  labs(\n    x = \"Year\",\n    y = \"Number of Lines\",\n    title = expression(paste(\"Number of Lines Spoken by \", italic(\"Friends \"), \"Characters from 1994 to 2004\"))\n  )\n\n\n\n\n\n\n\n\n\nLastly, I wanted to know how the number of lines spoken by each character changed over the lifetime of the show. Ross began the show with the largest number of lines, but was overtaken by Rachel in 1997. Phoebe consistently had the smallest number of lines, except for in 2001 when Ross’ number of lines dropped to below hers. Because the series began in September of 1994 and ended in May of 2004, these years have fewer lines overall compared to the intermediary years.\nHvitfeldt E (2021). friends: The Entire Transcript from Friends in Tidy Format. R package version 0.1.0.9000, https://github.com/EmilHvitfeldt/friends.\nData originally from the Character Mining repository."
  },
  {
    "objectID": "BirthdayParadox.html",
    "href": "BirthdayParadox.html",
    "title": "BirthdayParadox",
    "section": "",
    "text": "Show the code\nsimulate_birthdays &lt;- function(number) {\n  birthdays &lt;- sample(1:365, number, replace = TRUE)\n  any(duplicated(birthdays))\n}\n\nsimulate_birthdays(100)\n\n\n[1] TRUE"
  },
  {
    "objectID": "RedLights.html",
    "href": "RedLights.html",
    "title": "Red Lights",
    "section": "",
    "text": "Show the code\nlibrary(purrr)\nlibrary(ggplot2)\n\ngamble &lt;- function(initial_amount, goal_amount, p_win, win_multiplier) {\n  money &lt;- initial_amount\n  \n  while(money &gt; 0 & money &lt; goal_amount) {\n    money &lt;- money + ifelse(runif(1) &lt; p_win, win_multiplier, -1)\n  }\n  \n  # True means that the gambler met their goal amount. False means the gambler went bankrupt.\n  return(outcome = ifelse(money &lt;= 0, FALSE, TRUE))\n}\n\n\n\n\nShow the code\ngamble(10, 100, 0.1, 10)\n\n\n[1] FALSE\n\n\n\n\nShow the code\ngamble_simulation &lt;- function(n_runs = 1000, initial_amount, goal_amount, p_win,\n                              win_multiplier) {\n  win_prob &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win,\n                                        win_multiplier)) |&gt;\n    mean()\n  return(data.frame(win_prob = win_prob))\n}\n\n\n\n\nShow the code\ngamble_simulation(initial_amount = 10, goal_amount = 20, p_win = 0.5, win_multiplier = 10)\n\n\n  win_prob\n1        1\n\n\n\n\nShow the code\nchanging_p_win &lt;- function(n_runs = 1000, initial_amount = 10, goal_amount = 100, p_win,\n                                win_multiplier = 10) {\n  win_probs &lt;- map_dbl(p_win_values, function(p_win) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(p_win = p_win_values, win_prob = win_probs))\n}\n\n\n\n\nShow the code\np_win_values &lt;- seq(0.1, 0.9, by = 0.1)\n\nset.seed(47)\nchanging_p_win(p_win = p_win_values)\n\n\n  p_win win_prob\n1   0.1    0.224\n2   0.2    0.871\n3   0.3    0.967\n4   0.4    0.995\n5   0.5    0.999\n6   0.6    1.000\n7   0.7    1.000\n8   0.8    1.000\n9   0.9    1.000\n\n\n\n\nShow the code\nset.seed(47)\ngamble_simulation(initial_amount = 10, goal_amount = 100, p_win = 1/5000, win_multiplier = 10)\n\n\n  win_prob\n1        0\n\n\n\n\nShow the code\nchanging_win_multiplier &lt;- function(n_runs = 1000, initial_amount = 10, goal_amount = 100,\n                                    p_win = 0.1, win_multiplier) {\n  win_probs &lt;- map_dbl(win_multipliers, function(win_multiplier) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(win_multiplier = win_multipliers, win_prob = win_probs))\n}\n\n\n\n\nShow the code\nwin_multipliers &lt;- seq(1, 100, 10)\n\nset.seed(47)\nchanging_win_multiplier(win_multiplier = win_multipliers)\n\n\n   win_multiplier win_prob\n1               1    0.000\n2              11    0.315\n3              21    0.585\n4              31    0.633\n5              41    0.643\n6              51    0.636\n7              61    0.664\n8              71    0.637\n9              81    0.657\n10             91    0.647\n\n\n\n\nShow the code\nchanging_initial_amount &lt;- function(n_runs = 1000, initial_amount, goal_amount = 100,\n                                    p_win = 0.1, win_multiplier = 10) {\n  win_probs &lt;- map_dbl(initial_amounts, function(initial_amount) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(initial_amount = initial_amounts, win_prob = win_probs))\n}\n\n\n\n\nShow the code\ninitial_amounts &lt;- seq(1, 10, 1)\n\nset.seed(47)\nchanging_initial_amount(initial_amount = initial_amounts)\n\n\n   initial_amount win_prob\n1               1    0.021\n2               2    0.052\n3               3    0.072\n4               4    0.103\n5               5    0.110\n6               6    0.139\n7               7    0.142\n8               8    0.164\n9               9    0.169\n10             10    0.200"
  },
  {
    "objectID": "Slots.html",
    "href": "Slots.html",
    "title": "Slots Simulations",
    "section": "",
    "text": "The American Gaming Association recently reported that US casinos generated a record-breaking $49.78 billion dollars in 2024, a 0.8% increase over 2023 and the fourth-straight record-breaking year. Of this total, slot machines accounted for $36.06 billion.\nTo better understand slot machine win probabilities, I sought to create a function in R that simulates the probability that an individual would either reach their goal revenue or go bankrupt at a slot machine. The function takes an initial amount of money, a goal amount of money, a win probability, and a win multiplier value. To determine whether the individual will reach their goal amount, the function generates a random uniform number between 0 and 1. If that number is less than or equal to the win probability, the individual will gain revenue from that round, equal to the win multiplier amount. If the random uniform number is greater than the win probability, then they will lose $1. Then, the next round is played by generating another random uniform number. This cycle continues until either 1) the player reaches or exceeds their goal amount or 2) the player goes bankrupt.\n\nlibrary(purrr)\nlibrary(ggplot2)\n\ngamble &lt;- function(initial_amount, goal_amount, p_win, win_multiplier) {\n  money &lt;- initial_amount\n  \n  while(money &gt; 0 & money &lt; goal_amount) {\n    money &lt;- money + ifelse(runif(1) &lt;= p_win, win_multiplier, -1)\n  }\n  \n  # True means that the gambler met their goal amount. False means the gambler went bankrupt.\n  return(outcome = ifelse(money &lt;= 0, FALSE, TRUE))\n}\n\nSome assumptions engrained into this function are that 1) win probability does not change and 2) each win generates the same amount of revenue. Using map(), the function runs 1,000 times, calculating a win probability representing the fraction of times the player will meet their goal amount out of 1,000 slot machine sessions.\n\ngamble_simulation &lt;- function(n_runs = 1000, initial_amount, goal_amount, p_win,\n                              win_multiplier) {\n  win_prob &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win,\n                                        win_multiplier)) |&gt;\n    mean()\n  return(data.frame(goal_prob = win_prob))\n}\n\nI first wanted to investigate how changing the win probability impacts the player’s chances of meeting their goal amount. I set the simulation to give the player an initial amount of $10 and a goal amount of $100. I set the win multiplier to 10, meaning that for every slot session that is won, the player will gain $10.\n\nchanging_p_win &lt;- function(n_runs = 1000, initial_amount = 10, goal_amount = 100, p_win,\n                                win_multiplier = 10) {\n  win_probs &lt;- map_dbl(p_win_values, function(p_win) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(p_win = p_win_values, goal_prob = win_probs))\n}\n\nThe probability of reaching the goal amount was calculated for different win probabilities ranging from 0.1 to 0.9. Below are the results.\n\np_win_values &lt;- seq(0.1, 0.9, by = 0.1)\n\nset.seed(47)\np_win_variance &lt;- changing_p_win(p_win = p_win_values)\n\np_win_variance\n\n  p_win goal_prob\n1   0.1     0.224\n2   0.2     0.871\n3   0.3     0.967\n4   0.4     0.995\n5   0.5     0.999\n6   0.6     1.000\n7   0.7     1.000\n8   0.8     1.000\n9   0.9     1.000\n\nggplot(p_win_variance, aes(x = p_win, y = goal_prob)) +\n  geom_col(fill = \"steelblue3\") +\n  scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.1)) +\n  labs(\n    x = \"Probability of Winning One Round of Slots\",\n    y = \"Probability of Reaching Goal Amount\",\n    title = \"Reaching Goal Amount Depends on the Probability of Each Singular Round\"\n  )\n\n\n\n\n\n\n\n\nAs expected, a greater win probability leads to a greater probability of reaching the goal amount. At win probabilities of 0.4 or greater, the player is almost guaranteed to reach their goal amount.\nHowever, in reality, slot machine probabilities are much worse than even the 0.1 win probability tested above. One estimate is that win probabilities fall within a range of 1 in 5,000 to 1 in 34,000,000. To get a more accurate probability, I ran my simulation with a win probability of 1 in 5,000.\n\nset.seed(47)\ngamble_simulation(initial_amount = 10, goal_amount = 100, p_win = 0.002, win_multiplier = 10)\n\n  goal_prob\n1         0\n\n\nDecreasing the win probability to 1 in 5,000 changed the probability of reaching the goal value of $100 to 0, indicating it is extremely unlikely for the player to reach their goal in this scenario with this win probability.\nI was also curious as to how changing the win multiplier would change the probability of reaching the goal value. Because I wanted to be able to view trends in the data, I decided to set the win probability to 0.1 so that the player would have a visible chance at reaching their goal value. Again, I set the initial amount to $10 and the final amount to $100, running the simulation with different win multipliers ranging from 1 (the player wins $1 if they win the round) to 100 (the player wins $100 if they win the round).\n\nchanging_win_multiplier &lt;- function(n_runs = 1000, initial_amount = 10, goal_amount = 100,\n                                    p_win = 0.1, win_multiplier) {\n  win_probs &lt;- map_dbl(win_multipliers, function(win_multiplier) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(win_multiplier = win_multipliers, goal_prob = win_probs))\n}\n\n\nwin_multipliers &lt;- c(1, seq(10, 100, 10))\n\nset.seed(47)\nwin_multiplier_variance &lt;- changing_win_multiplier(win_multiplier = win_multipliers)\n\nwin_multiplier_variance\n\n   win_multiplier goal_prob\n1               1     0.000\n2              10     0.227\n3              20     0.583\n4              30     0.640\n5              40     0.635\n6              50     0.655\n7              60     0.639\n8              70     0.676\n9              80     0.651\n10             90     0.633\n11            100     0.637\n\nggplot(win_multiplier_variance, aes(x = win_multiplier, y = goal_prob)) +\n  geom_col(fill = \"steelblue3\") +\n  scale_x_continuous(breaks = c(1, seq(10, 100, 10))) +\n  labs(\n    x = \"Win Multiplier\",\n    y = \"Probability of Reaching Goal Amount\",\n    title = \"Win Multiplier Does Not Continually Increase the Probability of Reaching Goal\"\n  )\n\n\n\n\n\n\n\n\nWhile changing the win multiplier from 1 to 10 and 10 to 20 greatly increased the probability of reaching the goal amount, these same effects were not observed with further increases of the win multiplier. This indicates that once the win multiplier reaches a certain number, it no longer increases the probability of achieving the goal amount. In other words, at a certain point, adding more money to the “jackpot” will not help players reach their goals.\nLastly, I wanted to see how changing the initial amount of money impacted the probability of reaching the goal amount. For this simulation, I set the goal amount to $100, the win probability to 0.1, and the win multiplier to 10 and tested initial amounts ranging from $1 to $10.\n\nchanging_initial_amount &lt;- function(n_runs = 1000, initial_amount, goal_amount = 100,\n                                    p_win = 0.1, win_multiplier = 10) {\n  win_probs &lt;- map_dbl(initial_amounts, function(initial_amount) {\n    outcome &lt;- map_lgl(1:n_runs, ~gamble(initial_amount, goal_amount, p_win, win_multiplier))\n    mean(outcome)\n  })\n  \n  return(data.frame(initial_amount = initial_amounts, goal_prob = win_probs))\n}\n\n\ninitial_amounts &lt;- seq(1, 10, 1)\n\nset.seed(47)\ninitial_amount_variance &lt;- changing_initial_amount(initial_amount = initial_amounts)\n\ninitial_amount_variance\n\n   initial_amount goal_prob\n1               1     0.021\n2               2     0.052\n3               3     0.072\n4               4     0.103\n5               5     0.110\n6               6     0.139\n7               7     0.142\n8               8     0.164\n9               9     0.169\n10             10     0.200\n\nggplot(initial_amount_variance, aes(x = initial_amount, y = goal_prob)) +\n  geom_col(fill = \"steelblue3\") +\n  scale_x_continuous(breaks = seq(1, 10, 1)) +\n  labs(\n    x = \"Initial Amount ($)\",\n    y = \"Probability of Reaching Goal Amount\",\n    title = \"Increasing Initial Amount Increases the Probability of Reaching Goal\"\n  )\n\n\n\n\n\n\n\n\nThe data shows that increasing the initial amount of money for use at the slot machine increases the probability that a player will reach their goal amount. Thus, players that spend more are more likely to win because spending more money creates more opportunities to fall below the win probability and cash in on a win. This is part of why gambling and slots are so addictive-players believe that if they just try one more time and spend a little more, then in the next round they will hit the jackpot."
  },
  {
    "objectID": "Ethics.html",
    "href": "Ethics.html",
    "title": "Ethics in Data Science",
    "section": "",
    "text": "Background\nOn October 10, 2018, Reuters first reported that the AI tool used by Amazon to recruit employees made decisions biased against female applicants. The tool, designed in 2014, gave candidates scores ranging from 1 to 5 depending on the quality of their resumes. To generate these ratings, the model was trained using resumes that had been submitted to the company, and the hiring outcomes associated with these resumes, over a 10-year period.\nBy 2015, Amazon realized that the software exhibited gender-bias when rating employees. The root of the issue came from the composition of the training data. Since most Amazon employees were men, the tool was taught that the company preferred male applicants—and started downrating resumes with indications that the individual who submitted the resume was female. It penalized candidates whose resumes included the word “women’s” or who had attended all-women’s colleges.\nAfter the discovery, Amazon altered the model to treat these terms in a neutral way. However, the project created to design the model was ultimately disbanded at the beginning of 2017, partially due to the risk that the tool would discriminate against certain populations of candidates in a different way.\nAmazon insisted the software was only used as a recommendation for recruiters when evaluating candidates, and never as a sole determinant for hiring.\nTwo days later, on October 12, 2018, the American Civil Liberties Union published an article in response to the issue. In addition to examining the Amazon tool, the ACLU paper took a more wider perspective, saying that AI tools do not eliminate human bias but simply continue it through software. Gender discrimination is just one example–there is the inherent risk of discriminating against candidates from certain locations, who are members of certain ethnic community organizations, or who speak certain languages. Even video interviews analyzed by AI, a new strategy becoming increasingly popular to evaluate candidates, is subject to bias due to cultural differences in speech and eye movement patterns.\nWhile software that eliminates job candidates (deliberately or accidentally) based on attributes such as race, gender, or religion are illegal under Title VII, it is incredibly difficult to develop a strong case because individuals rejected from positions do not have the means to identify what led to their rejection. At the time of the article, the ACLU was in the process of challenging a federal law criminalizing the testing of employment websites for discrimination. The article finishes with a call to increase transparency around recruiting programs promote investigations into recruiting software by outside auditors.\nDiscussion\nTraining data bias\nOne strategy for minimizing the risk of incorporating bias into an AI tool is to train the tool on data that representative of the people to whom the algorithm will be applied. This decreases the chances that the algorithm makes judges new data a certain way simply because that data is different from the training data set. In the Amazon tool discrimination case, the software was trained on resumes previously submitted to the company over a 10-year period. These resumes reflected the gender imbalance in the tech industry in that they were predominantly from men. As a result, the software came to learn that Amazon preferred male employees, leading to the downrating of any resumes belonging to individuals who belonged to or contributed to women’s groups or associations. Amazon neglected to recognize this bias in the training data and only recognized the issue once the algorithm had already been in use and was generating biased results. In the case of trained models, it is crucial to know about the training data so that we can be aware of any potential biases integrated into the software.\nConsent for training data\nJust as obtaining consent is necessary before accessing private information in real-life, it is equally as important in the digital world. When people submit personal information online, they should be aware of who will has access to their information (eg., the general public, a singular company) and how their data may be used. Likewise, if a company promises an individual that their data will only be used for a specific purpose, then the company must uphold its promise and ensure that it only uses the data for that purpose. The Amazon recruiting tool was trained on resumes that had been submitted to the company over a 10-year period. The purpose of a resume is to help determine whether an individual is qualified for a particular position. Therefore, I would hypothesize that many of the individuals whose resumes were used to train the recruiting model were unaware that their information was being used for these purposes. I would also assume that, within the hundreds of thousands of people whose resumes were included in the data set, a significant number would feel uneasy about their information being used to train an AI recruiting tool. While these applicants never explicitly agreed with Amazon to only use their data to make a singular recruitment decision, there was an unspoken belief on the part of the candidates that that was what their information would be used for. Amazon violated this agreement by using their resumes to train its model. If Amazon had given applicants the option to sign an agreement stating they could use their data to train an AI recruiting model, then the company would have gained applicants’ consent to use their data. However, this option must be presented as strictly optional–pressuring applicants into agreeing to increase their chances of hiring eliminates the ability for applicants to decide how their data is used out of free will.\nIntentions for data usage\nAccording to the Data Values and Principles manifesto, data should be used “to improve life for our users, customers, organizations, and communities.” In creating their AI recruiting tool, the company’s goal was to reduce the amount of work needed to be done by company recruiters. While this sounds like it would improve the lives of recruiters, ultimately the software would lead to decreasing the number of recruiters working for Amazon. The company would benefit because it would cut costs on recruiting, leading to an increase in revenue or the ability to use that money elsewhere. In other words, the only entity that unequivocally benefits from AI recruiting is Amazon the company. In the long term, the tool has the potential to negatively impact recruiters (by putting them out of a job), and it definitely hurts applicants who were denied a job because the bias in the algorithm against their resumes. The tool clearly violated the manifesto, and more thought must go into any future tools in order to avoid the same negative repercussions.\nAlgorithm transparency\nAnother pillar of the Data Values and Principles manifesto is inviting criticism for the benefit of improving data science work. Having transparency at all levels of a project (data acquisition, model training, usage, etc.) creates the opportunity to identify errors and unintended consequences, discuss these issues, and move forward with a plan that creates the least amount of harm. As suggested by the ACLU, one strategy to mitigate the harmful effects of AI recruiting tools is to open the software up to auditing by neutral third parties. While there are currently auditors attempting to do this work, they often face barriers created by the company. This same secrecy was seen in the case of Amazon’s recruiting tool: according to employees, the bias was detected in 2015, but the Reuters article wasn’t published until 2018. Everything was kept under wraps inside Amazon. Amazon also declined to speak to Reuters about the issues with the tool. Keeping everything within the company meant that there were fewer eyes to examine the model and look for biases. It also prevented others (companies, organizations, data scientists, etc.) from learning from the model errors, which could have potentially led to the elimination of biases from similar tools used by other groups.\nReferences\nDastin, J. (2018), “Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women”, Reuters.\nGoodman, R. (2018), “Why Amazon’s Automated Hiring Tool Discriminated Against Women”, ACLU."
  },
  {
    "objectID": "OpenPolicing.html",
    "href": "OpenPolicing.html",
    "title": "Open Policing Project",
    "section": "",
    "text": "The Stanford Open Policing Project, published in Pierson et al. (2020), is a compilation of data acquired through millions of traffic stops made by law enforcement agencies across 42 different US states. Some of the data from these stops include records of the driver’s age. I wanted to compare the average age of stopped drivers across different California cities, as well as how this metric changed over time. To this end, I analyzed the data for all California cities included in the Open Policing Project that recorded driver age, calculating the average age of stopped drivers in each city for each year.\n\n\nShow the code\nlibrary(DBI)\ncon_traffic &lt;- DBI::dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\n\n\nShow the code\nSELECT \"San Francisco\" as city,\n  YEAR(date) AS stop_year,\n  AVG(subject_age) AS avg_age\nFROM ca_san_francisco_2020_04_01\nWHERE subject_age IS NOT NULL\n  AND date IS NOT NULL\nGROUP BY YEAR(date)\n\nUNION\n\nSELECT \"Bakersfield\" as city,\n  YEAR(date) AS stop_year,\n  AVG(subject_age) AS avg_age\nFROM ca_bakersfield_2020_04_01\nWHERE subject_age IS NOT NULL\n  AND date IS NOT NULL\nGROUP BY YEAR(date)\n\nUNION\n\nSELECT \"Long Beach\" as city,\n  YEAR(date) AS stop_year,\n  AVG(subject_age) AS avg_age\nFROM ca_long_beach_2020_04_01\nWHERE subject_age IS NOT NULL\n  AND date IS NOT NULL\nGROUP BY YEAR(date)\n\nUNION\n\nSELECT \"San Diego\" as city,\n  YEAR(date) AS stop_year,\n  AVG(subject_age) AS avg_age\nFROM ca_san_diego_2020_04_01\nWHERE subject_age IS NOT NULL\n  AND date IS NOT NULL\nGROUP BY YEAR(date)\n\nUNION\n\nSELECT \"Stockton\" as city,\n  YEAR(date) AS stop_year,\n  AVG(subject_age) AS avg_age\nFROM ca_stockton_2020_04_01\nWHERE subject_age IS NOT NULL\n  AND date IS NOT NULL\nGROUP BY YEAR(date)\n\nORDER BY city, stop_year;\n\n\n\n\nShow the code\ndbDisconnect(con_traffic, shutdown = TRUE)\n\n\n\n\nShow the code\nlibrary(ggplot2)\nggplot(stop_age_table, aes(x = city, y = avg_age)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of Average Stopped Driver Age by City\",\n       x = \"City\",\n       y = \"Average Stopped Driver Age\")\n\n\n\n\n\n\n\n\n\nThis plot is a box plot comparing the average stopped driver ages between California cities across all years from which data is available. The x-axis represents the 5 different cities (L to R: Bakersfield, Long Beach, San Diego, San Francisco, Stockton) and the y-axis shows the average driver age for each city. In each box, the vertical spread represents the variation in the average stopped driver age across different years. The middle line of each box marks the median average age, and the box boundaries represent the interquartile range. Individual points represent the average stopped driver age from a singular year. San Francisco had the highest average stopped driver age, followed by San Diego and then Bakersfield. Long Beach and Stockton had the lowest average stopped driver ages.\n\n\nShow the code\nstop_age_table |&gt;\n  ggplot(aes(x = stop_year, y = avg_age)) +\n  scale_x_continuous(breaks = seq(2007, 2018, by = 1)) +\n  geom_line(aes(color = city)) +\n  labs(\n    x = \"Year\",\n    y = \"Average Stopped Driver Age\",\n    color = \"City\",\n    title = \"Average Age of Stopped Drivers in California Cities\"\n  )\n\n\n\n\n\n\n\n\n\nThis plot is a line plot tracking the average stopped driver age each year in each of the 5 cities. The x-axis represents the year that the stop was made and the y-axis shows the average age of drivers stopped in that year. Each line represents a different city and connects the yearly averages to illustrate trends in driver age over time. The average age of stopped drivers in San Francisco increased by about 2 years between 2007 and 2016 in a roughly linear fashion. Bakersfield saw a steady increase of about 2 years between 2008 and 2010, then a drop to an average age of about 35.5 years that remained consistent until the last data point from 2018. The average stopped driver age in Long Beach also rose by approximately 2 years between 2008 and 2010, remaining stable until 2016 when there was about a 1 year jump in the 2017 data. Although the data for Stockton and San Diego only cover 4 and 3 years, respectively, the average stopped driver age is relatively stable.\nConclusions\nTogether, these two plots compare the average age of stopped drivers between California cities, as well as how that number has changed over time. Comparing the average age of stopped drivers to the average age of all drivers in these cities will lead to understanding whether certain age ranges are more prone to being stopped by law enforcement, or whether these ages simply reflect the average age of the driver population in that city. Seeing how the average stopped driver ages change over time compared to changes in the average driver population age, as well as the average general population age, will reveal whether the changes in average stopped driver age are due to changes in city demographics, or if there may be other underlying reasons for these changes.\n\n\n\n\nReferences\n\nPierson, Emma, Camelia Simoiu, Jan Overgoor, Sam Corbett-Davies, Daniel Jenson, Amy Shoemaker, Vignesh Ramachandran, et al. 2020. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour 4 (7): 736–45. https://doi.org/10.1038/s41562-020-0858-1."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Emmie Appl",
    "section": "Introduction",
    "text": "Introduction\nThe purpose of this website is to showcase my proficiency in R through a collection of completed projects. These projects span a variety of topics and applications, including data visualization, simulations, regular expression usage, and integration with database management systems. Through these examples, I aim to highlight both the breadth of my technical skills and my ability to apply them to real-world problems. I hope this serves as a demonstration of my readiness for data-driven roles that require analytical thinking, coding proficiency, and clear communication of results. The table below includes links and brief descriptions for each project. Feel free to take a look!"
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "Emmie Appl",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n\n\nProject Name\nDescription\n\n\n\n\nData Visualization:\n\nValentine’s Retail\nEuropean Drug Development\n\nVisualization of Valentine’s Day retail trends from 2010 to 2022 published by the National Retail Federation\nVisualization of data related to drugs considered for authorization in the European Union between 1995 and 2023\n\n\nString Analyses\nAnalysis of character dialogue from the entire Friends sitcom series\n\n\nSlots Simulation\nCreation of a program simulating a slot machine\n\n\nOpen Policing Project\nUses SQL code to access and visualize data from the Stanford Open Policing Project\n\n\nEthics\nOpinion piece of the ethics of AI-based hiring tools"
  },
  {
    "objectID": "Presentation.html#friends",
    "href": "Presentation.html#friends",
    "title": "String Analysis Presentation",
    "section": "Friends",
    "text": "Friends\n\n\nFriends (NBC, 1994–2004). Image source: Warner Bros. Television."
  },
  {
    "objectID": "Presentation.html#which-friends-character-has-the-most-lines",
    "href": "Presentation.html#which-friends-character-has-the-most-lines",
    "title": "String Analysis Presentation",
    "section": "Which Friends character has the most lines?",
    "text": "Which Friends character has the most lines?"
  },
  {
    "objectID": "Presentation.html#friends-data-set",
    "href": "Presentation.html#friends-data-set",
    "title": "String Analysis Presentation",
    "section": "Friends Data Set",
    "text": "Friends Data Set\nEvery utterance from Friends across all 10 seasons.\n\nData originally from the Character Mining repository from the Emory NLP Research Group.\nHvitfeldt E (2021). friends: The Entire Transcript from Friends in Tidy Format. R package version 0.1.0.9000, https://github.com/EmilHvitfeldt/friends."
  },
  {
    "objectID": "Presentation.html#friends-data-set-1",
    "href": "Presentation.html#friends-data-set-1",
    "title": "String Analysis Presentation",
    "section": "Friends Data Set",
    "text": "Friends Data Set\nlibrary(friends)\nlibrary(tidyverse)\n\nhead(friends)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nspeaker\nseason\nepisode\nscene\nutterance\n\n\n\n\nThere’s nothing to tell! He’s just some guy I work with!\nMonica Geller\n1\n1\n1\n1\n\n\nC’mon, you’re going out with the guy! There’s gotta be something wrong with him!\nJoey Tribbiani\n1\n1\n1\n2\n\n\nAll right Joey, be nice. So does he have a hump? A hump and a hairpiece?\nChandler Bing\n1\n1\n1\n3\n\n\nWait, does he eat chalk?\nPhoebe Buffay\n1\n1\n1\n4\n\n\n(They all stare, bemused.)\nScene Directions\n1\n1\n1\n5\n\n\nJust, ’cause, I don’t want her to go through what I went through with Carl- oh!\nPhoebe Buffay\n1\n1\n1\n6"
  },
  {
    "objectID": "Presentation.html#friends-data-set-2",
    "href": "Presentation.html#friends-data-set-2",
    "title": "String Analysis Presentation",
    "section": "Friends Data Set",
    "text": "Friends Data Set\non_a_break &lt;- friends |&gt;\n  filter(str_detect(text, \"on a break\"))\n\non_a_break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nspeaker\nseason\nepisode\nscene\nutterance\n\n\n\n\nWe were on a break!\nRachel Green\n3\n16\n13\n86\n\n\nWell, I guess he says that because they were on a break when it happened, that she should of forgiven him by now.\nJoey Tribbiani\n3\n17\n1\n16\n\n\nWe were on a break!!! Okay!! We were, we were…, yeah. Where are you? I’ll find you.\nRoss Geller\n3\n17\n9\n24\n\n\nWe were on a break!\nRoss Geller\n3\n17\n10\n38\n\n\nWell, if-if she thought they were on a break…\nRoss Geller\n4\n13\n12\n34\n\n\nFortunately, I won’t. And by the way, it seems to be perfectly clear that you were on a break.\nPassenger\n4\n24\n14\n11\n\n\nThis is fun. Hey Rach, remember that whole “We were on a break thing?” Well, I’m sorry, will you marry me?\nRoss Geller\n5\n15\n13\n11\n\n\nWhat about the time I said we were on a break?\nRoss Geller\n6\n5\n4\n23\n\n\nYeah. Oh I just wish we could not be married for a little bit! Y’know I just wish we could be like on a break!\nRachel Green\n6\n15\n10\n6\n\n\n’Cause you guys were on a break.\nBen Geller\n7\n16\n3\n5\n\n\nThat you and daddy were not on a break.\nBen Geller\n7\n16\n10\n5\n\n\nThat’s it?! You call that a fight? Come on! “We were on a break!” “No we weren’t!” What happened to you two?!\nPhoebe Buffay\n8\n8\n12\n13\n\n\nI dunno, well he got over the “We were on a break” thing really quickly.\nPhoebe Buffay\n9\n2\n4\n21\n\n\nAnd that’s why, no matter what mommy says, we really were on a break. Yes we were! Yes we were! Come here gorgeous. Oh! Look at you! You are the cutest little baby ever! You’re just a… a little bitty baby, you know that? But you’ve got… You’ve got big beautiful eyes… Yes you do… and a… and a big round belly. Big baby butt! I like big butts. I like big butts and I cannot lie / you other brothers can’t deny / when a girl walks in with an itty, bitty, waist / and a round thing in your face you get… Oh my God, Emma… you’re laughing! Oh my God, you’ve never done that before, have you? You never done that before… Daddy made you laugh, huh? Well, daddy and Sir Mix Alot… What? What? You… you wanna hear some more? Uhm… My anaconda don’t want none / unless you got buns hon… I’m a terrible father!\nRoss Geller\n9\n7\n1\n1\n\n\nThis is it. Unless we’re on a break.\nRoss Geller\n10\n18\n10\n20"
  },
  {
    "objectID": "Presentation.html#most-lines",
    "href": "Presentation.html#most-lines",
    "title": "String Analysis Presentation",
    "section": "Most Lines?",
    "text": "Most Lines?\nfriends |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_lines = n()) |&gt;\n  arrange(desc(num_lines)) |&gt;\n  head()\n\n\n\n\n\n\nspeaker\nnum_lines\n\n\n\n\nRachel Green\n9307\n\n\nRoss Geller\n9157\n\n\nChandler Bing\n8462\n\n\nMonica Geller\n8440\n\n\nJoey Tribbiani\n8171\n\n\nPhoebe Buffay\n7499"
  },
  {
    "objectID": "Presentation.html#most-questions",
    "href": "Presentation.html#most-questions",
    "title": "String Analysis Presentation",
    "section": "Most Questions?",
    "text": "Most Questions?\nquestions &lt;- friends |&gt;\n  select(text, speaker) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  mutate(totalquestions = str_count(text, \"\\\\?\")) |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_questions = sum(totalquestions))"
  },
  {
    "objectID": "Presentation.html#most-questions-1",
    "href": "Presentation.html#most-questions-1",
    "title": "String Analysis Presentation",
    "section": "Most Questions?",
    "text": "Most Questions?\nquestions &lt;- friends |&gt;\n  select(text, speaker) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  mutate(totalquestions = str_count(text, \"\\\\?\")) |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_questions = sum(totalquestions))\n  \n\ntotalnumber &lt;- friends |&gt;\n  select(text, speaker) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  mutate(totalphrases = str_count(text, \"\\\\?(?=\\\\s|$)|\\\\.(?=\\\\s|$)|\\\\!(?=\\\\s|$)\")) |&gt;\n  group_by(speaker) |&gt;\n  summarize(num_phrases = sum(totalphrases))"
  },
  {
    "objectID": "Presentation.html#who-has-the-most-linesyear",
    "href": "Presentation.html#who-has-the-most-linesyear",
    "title": "String Analysis Presentation",
    "section": "Who has the most lines/year?",
    "text": "Who has the most lines/year?\nfriends_info\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseason\nepisode\ntitle\ndirected_by\nwritten_by\nair_date\nus_views_millions\nimdb_rating\n\n\n\n\n1\n1\nThe Pilot\nJames Burrows\nDavid Crane & Marta Kauffman\n1994-09-22\n21.5\n8.3\n\n\n1\n2\nThe One with the Sonogram at the End\nJames Burrows\nDavid Crane & Marta Kauffman\n1994-09-29\n20.2\n8.1\n\n\n1\n3\nThe One with the Thumb\nJames Burrows\nJeffrey Astrof & Mike Sikowitz\n1994-10-06\n19.5\n8.2\n\n\n1\n4\nThe One with George Stephanopoulos\nJames Burrows\nAlexa Junge\n1994-10-13\n19.7\n8.1\n\n\n1\n5\nThe One with the East German Laundry Detergent\nPamela Fryman\nJeff Greenstein & Jeff Strauss\n1994-10-20\n18.6\n8.5\n\n\n1\n6\nThe One with the Butt\nArlene Sanford\nAdam Chase & Ira Ungerleider\n1994-10-27\n18.2\n8.1"
  },
  {
    "objectID": "Presentation.html#calculating-linesyear",
    "href": "Presentation.html#calculating-linesyear",
    "title": "String Analysis Presentation",
    "section": "Calculating lines/year",
    "text": "Calculating lines/year\nwithyear &lt;- left_join(friends, friends_info, by = c(\"season\", \"episode\")) |&gt;\n  mutate(year = as.factor(str_sub(air_date, 1, 4))) |&gt;\n  select(text, speaker, year) |&gt;\n  filter(str_detect(speaker, \"Rachel Green|Monica Geller|Phoebe Buffay|Joey Tribbiani$|Chandler Bing|Ross Geller\")) |&gt;\n  group_by(speaker, year) |&gt;\n  summarize(lines = n())\n\n\n\n\n\n\n\nspeaker\nyear\nlines\n\n\n\n\nChandler Bing\n1994\n337\n\n\nChandler Bing\n1995\n760\n\n\nChandler Bing\n1996\n858\n\n\nChandler Bing\n1997\n897\n\n\nChandler Bing\n1998\n895\n\n\nChandler Bing\n1999\n975"
  },
  {
    "objectID": "Presentation.html#visualizing-the-data",
    "href": "Presentation.html#visualizing-the-data",
    "title": "String Analysis Presentation",
    "section": "Visualizing the Data",
    "text": "Visualizing the Data\n\nggplot(withyear, aes(x = year, y = lines, group = speaker)) +\n  geom_line(aes(color = speaker)) +\n  labs(\n    x = \"Year\",\n    y = \"Number of Lines\",\n    title = expression(paste(\"Number of Lines Spoken by \", italic(\"Friends \"), \"Characters from 1994 to 2004\"))\n  )"
  },
  {
    "objectID": "Presentation.html#friends-data-table",
    "href": "Presentation.html#friends-data-table",
    "title": "String Analysis Presentation",
    "section": "Friends Data Table",
    "text": "Friends Data Table\nlibrary(friends)\nlibrary(tidyverse)\n\nhead(friends)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nspeaker\nseason\nepisode\nscene\nutterance\n\n\n\n\nThere’s nothing to tell! He’s just some guy I work with!\nMonica Geller\n1\n1\n1\n1\n\n\nC’mon, you’re going out with the guy! There’s gotta be something wrong with him!\nJoey Tribbiani\n1\n1\n1\n2\n\n\nAll right Joey, be nice. So does he have a hump? A hump and a hairpiece?\nChandler Bing\n1\n1\n1\n3\n\n\nWait, does he eat chalk?\nPhoebe Buffay\n1\n1\n1\n4\n\n\n(They all stare, bemused.)\nScene Directions\n1\n1\n1\n5\n\n\nJust, ’cause, I don’t want her to go through what I went through with Carl- oh!\nPhoebe Buffay\n1\n1\n1\n6"
  },
  {
    "objectID": "Presentation.html#on-a-break",
    "href": "Presentation.html#on-a-break",
    "title": "String Analysis Presentation",
    "section": "On a break?",
    "text": "On a break?\non_a_break &lt;- friends |&gt;\n  filter(str_detect(text, \"on a break\"))\n\non_a_break\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nspeaker\nseason\nepisode\nscene\nutterance\n\n\n\n\nWe were on a break!\nRachel Green\n3\n16\n13\n86\n\n\nWell, I guess he says that because they were on a break when it happened, that she should of forgiven him by now.\nJoey Tribbiani\n3\n17\n1\n16\n\n\nWe were on a break!!! Okay!! We were, we were…, yeah. Where are you? I’ll find you.\nRoss Geller\n3\n17\n9\n24\n\n\nWe were on a break!\nRoss Geller\n3\n17\n10\n38\n\n\nWell, if-if she thought they were on a break…\nRoss Geller\n4\n13\n12\n34\n\n\nFortunately, I won’t. And by the way, it seems to be perfectly clear that you were on a break.\nPassenger\n4\n24\n14\n11\n\n\nThis is fun. Hey Rach, remember that whole “We were on a break thing?” Well, I’m sorry, will you marry me?\nRoss Geller\n5\n15\n13\n11\n\n\nWhat about the time I said we were on a break?\nRoss Geller\n6\n5\n4\n23\n\n\nYeah. Oh I just wish we could not be married for a little bit! Y’know I just wish we could be like on a break!\nRachel Green\n6\n15\n10\n6\n\n\n’Cause you guys were on a break.\nBen Geller\n7\n16\n3\n5\n\n\nThat you and daddy were not on a break.\nBen Geller\n7\n16\n10\n5\n\n\nThat’s it?! You call that a fight? Come on! “We were on a break!” “No we weren’t!” What happened to you two?!\nPhoebe Buffay\n8\n8\n12\n13\n\n\nI dunno, well he got over the “We were on a break” thing really quickly.\nPhoebe Buffay\n9\n2\n4\n21\n\n\nAnd that’s why, no matter what mommy says, we really were on a break. Yes we were! Yes we were! Come here gorgeous. Oh! Look at you! You are the cutest little baby ever! You’re just a… a little bitty baby, you know that? But you’ve got… You’ve got big beautiful eyes… Yes you do… and a… and a big round belly. Big baby butt! I like big butts. I like big butts and I cannot lie / you other brothers can’t deny / when a girl walks in with an itty, bitty, waist / and a round thing in your face you get… Oh my God, Emma… you’re laughing! Oh my God, you’ve never done that before, have you? You never done that before… Daddy made you laugh, huh? Well, daddy and Sir Mix Alot… What? What? You… you wanna hear some more? Uhm… My anaconda don’t want none / unless you got buns hon… I’m a terrible father!\nRoss Geller\n9\n7\n1\n1\n\n\nThis is it. Unless we’re on a break.\nRoss Geller\n10\n18\n10\n20"
  }
]