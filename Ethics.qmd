---
title: "Ethics in Data Science"
description: |
  Exploring the Ethics of AI Recruiting
author: Emmie Appl
date: April 16, 2025
format: html
execute:
  warning: false
  message: false
---

**Background**

On October 10, 2018, Reuters [first reported](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G/) that the AI tool used by Amazon to recruit employees made decisions biased against female applicants. The tool, designed in 2014, gave candidates scores ranging from 1 to 5 depending on the quality of their resumes. To generate these ratings, the model was trained using resumes that had been submitted to the company, and the hiring outcomes associated with these resumes, over a 10-year period.

By 2015, Amazon realized that the software exhibited gender-bias when rating employees. The root of the issue came from the composition of the training data. Since most Amazon employees were men, the tool was taught that the company preferred male applicants—and started downrating resumes with indications that the individual who submitted the resume was female. It penalized candidates whose resumes included the word "women's" or who had attended all-women's colleges.

After the discovery, Amazon altered the model to treat these terms in a neutral way. However, the project created to design the model was ultimately disbanded at the beginning of 2017, partially due to the risk that the tool would discriminate against certain populations of candidates in a different way.

Amazon insisted the software was only used as a recommendation for recruiters when evaluating candidates, and never as a sole determinant for hiring.

Two days later, on October 12, 2018, the American Civil Liberties Union [published an article](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against) in response to the issue. In addition to examining the Amazon tool, the ACLU paper took a more wider perspective, saying that AI tools do not eliminate human bias but simply continue it through software. Gender discrimination is just one example–there is the inherent risk of discriminating against candidates from certain locations, who are members of certain ethnic community organizations, or who speak certain languages. Even video interviews analyzed by AI, a new strategy becoming increasingly popular to evaluate candidates, is subject to bias due to cultural differences in speech and eye movement patterns.

While software that eliminates job candidates (deliberately or accidentally) based on attributes such as race, gender, or religion are illegal under Title VII, it is incredibly difficult to develop a strong case because individuals rejected from positions do not have the means to identify what led to their rejection. At the time of the article, the ACLU was in the process of challenging a federal law criminalizing the testing of employment websites for discrimination. The article finishes with a call to increase transparency around recruiting programs promote investigations into recruiting software by outside auditors.

**Discussion**

*Training data bias*

One strategy for minimizing the risk of incorporating bias into an AI tool is to train the tool on data that representative of the people to whom the algorithm will be applied. This decreases the chances that the algorithm makes judges new data a certain way simply because that data is different from the training data set. In the Amazon tool discrimination case, the software was trained on resumes previously submitted to the company over a 10-year period. These resumes reflected the gender imbalance in the tech industry in that they were predominantly from men. As a result, the software came to learn that Amazon preferred male employees, leading to the downrating of any resumes belonging to individuals who belonged to or contributed to women's groups or associations. Amazon neglected to recognize this bias in the training data and only recognized the issue once the algorithm had already been in use and was generating biased results. In the case of trained models, it is crucial to know about the training data so that we can be aware of any potential biases integrated into the software.

*Consent for training data*

Just as obtaining consent is necessary before accessing private information in real-life, it is equally as important in the digital world. When people submit personal information online, they should be aware of who will has access to their information (eg., the general public, a singular company) and how their data may be used. Likewise, if a company promises an individual that their data will only be used for a specific purpose, then the company must uphold its promise and ensure that it only uses the data for that purpose. The Amazon recruiting tool was trained on resumes that had been submitted to the company over a 10-year period. The purpose of a resume is to help determine whether an individual is qualified for a particular position. Therefore, I would hypothesize that many of the individuals whose resumes were used to train the recruiting model were unaware that their information was being used for these purposes. I would also assume that, within the hundreds of thousands of people whose resumes were included in the data set, a significant number would feel uneasy about their information being used to train an AI recruiting tool. While these applicants never explicitly agreed with Amazon to only use their data to make a singular recruitment decision, there was an unspoken belief on the part of the candidates that that was what their information would be used for. Amazon violated this agreement by using their resumes to train its model. If Amazon had given applicants the option to sign an agreement stating they could use their data to train an AI recruiting model, then the company would have gained applicants' consent to use their data. However, this option must be presented as strictly optional–pressuring applicants into agreeing to increase their chances of hiring eliminates the ability for applicants to decide how their data is used out of free will.

*Intentions for data usage*

According to the [Data Values and Principles manifesto](https://datapractices.org/manifesto/), data should be used "to improve life for our users, customers, organizations, and communities." In creating their AI recruiting tool, the company's goal was to reduce the amount of work needed to be done by company recruiters. While this sounds like it would improve the lives of recruiters, ultimately the software would lead to decreasing the number of recruiters working for Amazon. The company would benefit because it would cut costs on recruiting, leading to an increase in revenue or the ability to use that money elsewhere. In other words, the only entity that unequivocally benefits from AI recruiting is Amazon the company. In the long term, the tool has the potential to negatively impact recruiters (by putting them out of a job), and it definitely hurts applicants who were denied a job because the bias in the algorithm against their resumes. The tool clearly violated the manifesto, and more thought must go into any future tools in order to avoid the same negative repercussions.

*Algorithm transparency*

Another pillar of the Data Values and Principles manifesto is inviting criticism for the benefit of improving data science work. Having transparency at all levels of a project (data acquisition, model training, usage, etc.) creates the opportunity to identify errors and unintended consequences, discuss these issues, and move forward with a plan that creates the least amount of harm. As suggested by the ACLU, one strategy to mitigate the harmful effects of AI recruiting tools is to open the software up to auditing by neutral third parties. While there are currently auditors attempting to do this work, they often face barriers created by the company. This same secrecy was seen in the case of Amazon's recruiting tool: according to employees, the bias was detected in 2015, but the Reuters article wasn't published until 2018. Everything was kept under wraps inside Amazon. Amazon also declined to speak to Reuters about the issues with the tool. Keeping everything within the company meant that there were fewer eyes to examine the model and look for biases. It also prevented others (companies, organizations, data scientists, etc.) from learning from the model errors, which could have potentially led to the elimination of biases from similar tools used by other groups.

**References**

Dastin, J. (2018), “[Amazon Scraps Secret AI Recruiting Tool that Showed Bias Against Women](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G)", Reuters.

Goodman, R. (2018), "[Why Amazon’s Automated Hiring Tool Discriminated Against Women](https://www.aclu.org/news/womens-rights/why-amazons-automated-hiring-tool-discriminated-against)", ACLU.
